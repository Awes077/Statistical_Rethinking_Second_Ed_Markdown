---
title: "Chapter_4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
```

# Chapter 4 - Why Normal distributions are normal

We start with a coin flip and movement example. Line up 1000 people on the center of a football field, have them flip a coin over and over. For each heads they get they step to the left of the midline, for each tails they step to the right of the midline. While we can't predict how far any given individual will have stepped in either direction, we can 100% predict the distribution of distances across those 1000 people. It will be roughly normally distributed. This is, more or less I think, because of the maximum entropy principle. Most people will on average not move, because there are so many permutations of some number of flips that sum to 0 when we can move one step left or one step right. There are slightly fewer that sum to 1 step left or 1 step right. There are slightly fewer still that sum to 2 steps, and so on. Thus even when the process is binomial, we can summarize its outcomes with a normal distribution.

## 4.1.1 - Normal by Addition

So we simulate the coin flip example using R, 16 steps between 1 and -1, replicated across 1000 of our supposed friends. When we sum the 16 steps and plot those 1000 sums, they are ALWAYS normally distributed. So why?

The core logic has to do with random draws from any distribution. Every draw represents a random deviation from the underlying mean. The more draws we have the more likely it is some of those deviations will cancel each other out. So on average, most sets will almost completely cancel out so that, by definition, on average everyone is near the mean. But some will not and so we see the characteristic bell curve for those sums. Thus addition of results from any underlying distribution (we used the uniform distribution for this) will always be Gaussian.

To carry this further, there are many many ways to cancel out positive or negative deviations. But there are very few ways (ie permutations of our events) that yield large negative or positive sums. Thus we see our inevitable Gaussian. So that brings us back to the max entropy perspective. Our central values make the fewest assumptions about our underlying events that constitute the sums. Large values can only be made by many steps in one direction or a few large steps with few small steps in the opposite. So you can think of it like, there is an equal probability of any given permutation. But permutations can have the same sum. And as we move closer to the center, we have more and more permutations that have the same sum - thus, given they are all equally probable, we would expect certain sums to appear much more frequently. So the normal distribution emerges. Heck yeah.

```{r Normal by addition}
#setion 4.1.1 - Normal by addition

#simulating the coin toss and step example for explaining the normal distribution.

#16 coin flips, steps between -1 and 1, 1000 people doing it. Always normal.
pos <- replicate(1000, sum(runif(16, -1,1)))

hist(pos)


```

## 4.1.2 - Normal by multiplication

Now we look at multiplicative effects. Frame story is multiple loci affecting a growth rate, and each has a multiplicative effect with the others. When we sample 12 of them from 1 to 1.1 and multiply them, then replicate 10,000 times, we find that our growth rates are normally distributed around 1.8ish? Let's find out why! Cause I genuinely don't know so.

AHH FUCK it's because in this example, the multiplicative elements are quite small. Multiplying numbers with small differences ain't all that different from adding them, and so we could actually substitute a sort of additive approximation. Wonder if this breaks down then if differences, e.g. effects of each multiplicative element are larger, even if still roughly equal?


```{r Normal by multiplication}

#section 4.1.2 - Normal by multiplication

#looking at how multiplicative effects rather than additive effects 


#example here is multiplicative effects of genes on growth rate. So we sample 12 growth rates from 1 to 1.1, where 1 
#indictes no growth and 1.1 indicates a 10% increase. We then multiply these loci together. We replicate 10000 times and
#plot, and find rates are roughly nomrally distributed.
growth <- replicate(10000, prod(1+runif(12,0,0.1)))
dens(growth, norm.comp = T)


#now let's test with larger effects.

big <- replicate(10000, prod(1+runif(12, 0,0.5)))
dens(big, norm.comp = T)

#AHHH so it's not normally distributed now punk! This makes sense though, based on McElreath's explanation. We see that
#since effects are larger now, multiplying them deviates from an additive process much more strongly. Thus we lose the
#normally distributed pattern. But I fucking bet if we log transform this shit it's normal again ayyy??

```

## 4.1.3 Normal by log multiplication

Unsurprisingly, while larger effects do not produce Gaussian distributions when just multiplied together, they are normally distributed when multiplied together on a log scale. Collapsing those differences as ever, such that we could represent them as additive processes without too much loss.

```{r Normal by log multiplication}
log.big <- replicate(10000, log(prod(1+runif(12,0,0.5))))
dens(log.big, norm.comp = T)
```


## 4.1.4 Justifying Uses of Normal Distribution

We are going to justify using normal distributions as the basis for damn near all models using both ontological and epistemological reasoning. No idea why that's important but hey let's see where he goes with this yeah?

### 4.1.4.1 Ontological Justification

Basically we argue here that we use the Normal distribution becasue it is incredibly common. By adding together deviations or fluctuations, which is what normal distributions do, we shed idiosyncratic information about processes. This means that normality is a solid approximation for many different types of process and can be used widely. All that normality requires is a mean and spread around that mean. So long as a process can be summarized sufficiently with those two components, normality will REIGN SUPREME.

There are of course other consistent patterns that are necessary for other data types - exponential distributions of waiting times, Poisson distributions of frequencies of counts, etc. All of these belong to the exponetial family of distributions and we'll cover them later.

### 4.1.4.2 Epistemological Justification

The normal distribution has the loosest set of assumptions for the underlying process or data. Basically, it represents a specific 'state of ignorance' about our data or its underlying distrbution. If all we are comfortable saying about our data is it's mean and variance (or that it has a mean and finite variance around that mean), then the normal distribution makes no additional assumptions. Thus it is the best reflection we have for our current understanding.

As we learn more about a given system, we may be able to make more justified assumptions and thus to make different choices about distributions. But so long as we have no additional assumptions, the normal distribution is the one that can be realized in the most ways without introducing unjustified constraints or parameters.

As ever, he points out that models are tools or golems. They do not define our assumptions about the system, we define their assumptions about inference. And so we need not be bound by the assumptions of the model in our understanding of the system. But it is good to keep those limitations in mind as we draw further conclusions etc.


#### Overthinking - Gaussian distribution

Not going to go into detail about his pdf vs pmf work here, but I will use this as an opportunity to talk a bit about the probability density function for a Gaussian. In particular the pi. e makes a bit more sense, and acts as a base that scales sigma. But pi is weirder. Basically it acts as our normalizing constant that ensures the area under the curve always sums to 1. To understand this, you have to understand the Gaussian integral:

$\int e^{-x^{2}} dx = \sqrt{\pi}$

So basically, we know that the total area under an exponential to the power of -x^2 = sqrt(pi). Thus we can standardize our area in the Gaussian by pi (along with 2 sigma) to guarantee that our pdf sums to 1 across its entire area.

Understanding the Gaussian integral?? that's a whole other thing. Dont know that I can talk through that as it requires understanding of the relationship between e, volume of circles, pi, and I think some multivariate calculus. But hey that's life right? Gotta be limited somewhere.



## 4.2 A Language for Describing Models

Laying out some basic linguistic conventions for describing models and data.

1. A set of measurements that we want to predict or understand are called 'outcome' variables.

2. For each of these observations, we define a likelihood function that describes the plausibility of individual data points. For linear models, this is always Gaussian.

3. Next we have observations we want to use to predict these outcomes. We call these predictors or predictor variables.

4. We relate the shape of the likelihood model to the predictor variables - its precise location and variance as well as other properties if it has them. Thus we can define all parameters of the likelihood model using our predictors. 

5. Lastly, we choose priors for each of those parameters. These define the initial information on the state of the model.


Then we can write the model out in very mathy terms, like:

$outcome_{i} \sim Normal(\mu_{i}, \sigma)$

$\mu_{i} = \beta \times predictor_{i}$

$\beta \sim Normal(0,10)$

$\sigma \sim HalfCauchy(0,1)$

Very proud of myself for that very simple latex, but anyways. Here we say that our outcome is normally distributed around $\mu$ with variance $\sigma$. $\mu$ is then a function of our predictor times a coefficient $\beta$. The prior on $\beta$ is a normal distribution centered at 0 with a standard deviation of 10, while the prior on our variance is a Half Cauchy with 0 and 1 as the parameters. I don't know what the parameters are for a half cauchy, should probably look that up? First is a location parameter $\mu$ and then a scale parameter $\sigma$. Basically like half of a normal distribution when parameterized this way.

Love that he lays out the ridiculousness (to some degree) of how we refer to regression, multiple regression, ANOVA, ANCOVA, etc. when in reality they are just extensions of this model structure. It's always mapping an outcome onto predictors using some likelihood function. Love to see it.

### 4.2.1 Re-describing the globe tossing model

The example of tossing a globe to assess the proportion of water on said globe can be re-expressed in the language above:

$w \sim Binomial(n, p)$
$p \sim Uniform(0,1)$

Here we have a binomial likelihood mapping our outcomes, $w$ the number of tosses that come up water, onto $n$ samples (tosses) with a probability of water, $p$. The prior on $p$ is then set as a uniform distribution from 0 to 1. 

Note that the $sim$ symbol implies that all of these are stochastic, which is to say that no outcome or probability drawn from the prior is known with certainty. There are more or less plausible results, but overall the system is probabilistic rather than deterministic. 


#### Overthinking: From model to Bayes theorem

Coming from the model above, we can insert it into Bayes' classic equation:

$Pr(p\mid w,n) = \frac{Binomial(w\mid n,p) Uniform(p\mid 0,1)}{\int Binomial(w\mid n,p) Uniform(p\mid 0,1)dp}$


This is effectively just our grid approximation from earlier. We calculate the likelihoods for each value on our grid, multiply by the prior, then divide by the sum of that across all values on the grid to get our posterior to sum to 1. Simple as calculus.


## 4.3 A Gaussian Model of Height


We are going to build a model of height using the data from Nancy Howell's work on the !Kung San.

```{r Gaussian model of Height - starter}
data("Howell1")

d <- Howell1

str(d)
```


Technically speaking, there are an infinite number of potential Gaussian distributions, with some mean and variance. We are interested then in finding the one that is most plausible given the data we have. What is the 'best fit' mean and variance that would be most likely to generate this data? And, in a Bayesian framework, how certain are we about those parameters?

The general form of our height model is just:


$h_{i} \sim Normal(\mu, \sigma)$


Just some text here

***

##### Rethinking: Independent and Identically Distributed

Basically, all of our models are going to assume that data within levels are iid, which is to say, they are independent draws from the same underlying distribution. So identically distributed, in that they come from a distribution with the same fixed parameters, but are independent draws from that distribution. An example of non-independence would be physical measurements of related individuals. 

A truly great point here that this assumption - the model assumption of iid as defined by a likelihood function - is an epistemological assumption not an ontological one. It is a tool we use for our understanding, an approximation of reality - a small world assumption of the 'golem' we are building. It is not something inherently true about the world. Otherwise, we engage in what ET Jaynes called the 'mind projection fallacy' where we assume that our understanding of a thing, the structure we impose on it to understand it, must define the thing itself. Signs and signifiers y'all. Maps, territories, you know the drill.




***


Also a great point from Ch 1 about how many process models can correspond to the same statistical model. Really fucking me up here Rich, like honestly though, love it. Anyways, this is all to say, we end up with a Gaussian model and associated priors such that:

$h_{i} \sim Normal(\mu, \sigma)$ - Likelihood
$\mu \sim Normal(178, 20)$ - Prior for $\mu$
$\sigma \sim Uniform(0,50)$ - Prior for $\sigma$

So the central value (mean) we chose for our prior on $\mu$, 178 cm, is literally just McElreath's height, the idea being we choose a roughly average central value and then a wide variance. So the mean could range anywhere from 138 to 218 cm based on our 95% intervals with a standard deviation of 20 cm. So we have plenty of room to move within that but don't assume it's uniform, there are still more and less plausible values for mean height. Here we use some level of everyday 'expertise' to inform our prior. We can't always do that obviously.

Now we'll plot that prior to get an idea of what it is saying out the distribution of possible means - what means are plausible vs not plausible and so on - as this is what will inform the 'completeness' of our posterior sampling.

```{r Prior predictive checks}
curve( dnorm( x , 178 , 20) , from=100, to=250)

curve( dunif( x , 0 , 50) , from=10 , to=60 )
```


Note that while our mean $\mu$ has a normal prior informed by human height, our sigma is uniform as heck - just a straight fuckin brick of probability from 0 to 50. What an absolute unit.

Now we want to sample from the prior. This is like sampling from our posterior before, but now looking just at what information we've not-so-tacitly coded into our priors.

```{r More Prior Predictives}
sample_mu <- rnorm(1e4, 178, 20) #sample 10,000 means from our prior of 178,20
sample_sigma <- runif(1e4, 0,50) #sample 10,000 variances from our uniform prior, 0 to 50.

prior_h <- rnorm(1e4, sample_mu, sample_sigma) # so here I think we basically draw like...10,000 samples from each of our 10,000
#simulated parameterizations? Like we have two vectors of 10,000 mus and sigmas. We then sample 10,000 from each yeah? No, cuz it ends up still being 10,000 long it looks like, so it must just sample one from each then? Is that right? Doesn't seem to be much in terms of documentaiton of that behavior, so i'll just test it in the console.

#So yeah rnorm will draw a single point form each pair from the vectors. If you ask for more n than there are in each vector, it just
#loops back and starts again. So like, if I ask for 3 values, but i only provide 2 means and 2 variances, it gives me a draw from pair 1, then one from pair 2, then another from pair 1. Nice.

dens(prior_h) 
```


This plot represents the expected distribution of heights, averaged over our prior. So basically, we draw 10,000 pairs of $\mu$ and $\sigma$ from the prior, then simulate a single data point from each of those 10,000 combinations. This gives us 10,000 draws from what are basically 10,000 different distributions informed by the priors we have set.


This seems like a very important idea for exploring how our priors impact our eventual inference I would think. If this seems to bias us towards specific values, then that's probably v bad. by and large though, this seems pretty broad and sensible based on what we no a priori about human height.

---

##### Dropping $\epsilon$


McElreath notes that lots of texts teach regression based models in the basic algebraic form:

$h_{i} = \mu + \epsilon_{i}$
$\epsilon_{i} \sim Normal(0, \sigma)$

Where $\epsilon$ represents an error term that is normally distributed. While I don't see a huge issue with that personally, he points out that this doesn't generalize well to non-Gaussian models. So basically, when we want to work with like, a Binomial or Poisson model, then our $\epsilon$ doesn't really do the job we need. So he opts for the more easily generalized notation:

$h_{i} \sim Normal(\mu, \sigma)$

Since we can just swap out Normal for whatever else we want and it remains roughly the same I think. Particularly because what we are really then doing is making $\mu$ or $\lambda$ or what have you a linear function of some predictors, which are then potentially transformed to the scale of our prediction - using the link function. So it gets us towards GLMs much quicker.



---

##### Overthinking: Model Definition to Bayes' Theorem

Here we are just going to insert our parameter values from the priors into the actual Bayes equation, simple as can be:
$$
Pr(\mu, \sigma \mid h) = \frac{\prod_{i}Normal(h_{i}\mid \mu,\sigma)Normal(\mu\mid 178,20)Uniform(\sigma\mid 0,50)}{\int\int\prod_{i}Normal(h_{i}\mid \mu,\sigma)Normal(\mu\mid 178,20)Uniform(\sigma\mid 0,50)d\mu d\sigma}
$$

Once again, this looks a lil wild, but it's really just pluggin in our priors. So basically, our posterior probability for a given parameter combination, given our data, ($Pr(\mu, \sigma \mid  h)$) is equal to the likelihood of our data under that model ($Normal(h_{i}\mid \mu,\sigma)$) times the prior probability of those model parameters given our prior ($Normal(\mu\mid 178,20)Uniform(\sigma\mid 0,50)$). We then standardize that by the likelihood of our data given ANY parameterization of the current model structure, which is the denominator, just integrated our posterior over $\mu$ and $\sigma$. 

Also worth noting, he points out that we are going to do the calculations on the log scale, so log likelihoods, which puts us into logarithmic addition rather than multiplication. Noice.


Also be careful with spaces and inline latex. Had a weird issue just now with that, so just like...keep an eye out.
---


### 4.3.3 Grid Approximation of the Posterior

So first we are going to set up a full grid approximation of the posterior for our two parameters - $\mu$ and $\sigma$. 

```{r Grid Approximation of Posterior}
d2 <- d[ d$age >= 18 , ]

mu.list <- seq(from = 140, to = 160, length.out = 200)
sigma.list <- seq(from=4, to=9, length.out=200)

post <- expand.grid(mu=mu.list, sigma=sigma.list)



post$LL <- sapply(1:nrow(post), function(i) sum(dnorm( d2$height, mean=post$mu[i], sd=post$sigma[i], log=T)))

post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE)+dunif(post$sigma, 0, 500, T)

post$prob <- exp(post$prod - max(post$prod))


contour_xyz(post$mu, post$sigma, post$prob)

image_xyz(post$mu, post$sigma, post$prob)
```


### 4.3.4 Sampling from the Posterior

As ever, the grid approximation above (and even the eventual quardratic approximation) is not realistic for most real modelling problems. Instead, we usually have samples we sumamrize to characterize the posterior distribution. This is no different that previous samplings of the posterior except that now we have two parameters - meaning we want to sample combinations of them. For now, we start by sampling row numbers based on post$prob. Then we just look at the parameter values.

```{r Sampling from the Posterior}

#randomly sample rows based on post$prob
sample.rows <- sample(1:nrow(post),size=1e4, replace=T, prob = post$prob)

#pull parameter values in those rows
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]

#plot against each other, the more ponts drawn from a given area, the darker the overlay on them. We set alpha low so as we sample the same locations repeatedly
#they get darker and darker.
plot(sample.mu, sample.sigma, cex=0.5, pch=16, col=col.alpha(rangi2, 0.1))

#here's a marginal plot of the density for mu and sigma, marginal in the sense that we take all the values of mu - basically averaged over all the values for sigma, and vice versa.

dens(sample.mu)
dens(sample.sigma)

#notice that both are roughly normal, but that sigma has a slight right tail

HPDI(sample.mu)
HPDI(sample.sigma)

```

Note that when we start using the map function in later sections, this is the quadratic approximation we talked about earlier it looks like.

###### Overthinking: Sample size and the normality of $\sigma$

Basically this demonstrates that, because we have a normal prior on $\mu$, our posterior will always be Gaussian more or less. But $\sigma$ is a different story. In particular, since variance must always be positive, we are always more uncertain about how large $\sigma$ is than how small $\sigma$ is. The book gives the example of, imagine we estimate $\sigma$ to be close to 0. We know it cannot be much smaller, since it cannot be negative, but it could be much larger. This only becomes a problem with smaller sample sizes. So if we take a small subset of our data (about 20 data points) and refit, we'll see that we end up with an even more pronounced tail.


```{r Small Sample normality}
d3 <- sample(d2$height, size=20)

mu.list <- seq(from = 140, to = 160, length.out = 200)
sigma.list <- seq(from=4, to=9, length.out=200)

post2 <- expand.grid(mu=mu.list, sigma=sigma.list)




post2$LL <- sapply(1:nrow(post2), function(i) sum(dnorm( d3, mean=post2$mu[i], sd=post2$sigma[i], log=T)))

post2$prod <- post2$LL + dnorm(post2$mu, 178, 20, TRUE)+dunif(post2$sigma, 0, 500, T)

post2$prob <- exp(post2$prod - max(post2$prod))


sample2.rows <- sample(1:nrow(post2), size=1e4, replace=T, prob=post2$prob)

sample2.mu <- post2$mu[sample2.rows]
sample2.sigma <- post2$sigma[sample2.rows]

plot(sample2.mu, sample2.sigma, cex=0.5, col=col.alpha(rangi2, 0.1),
     xlab="mu", ylab='sigma', pch=16)

dens(sample2.sigma, norm.comp = T)



```

### 4.3.5 Fitting the model with map

The basic idea here is to try and approximate the maximum a posteriori (MAP), basically the peak of the posterior. Not a lot of details on actual quadratic approximation, but it looks like its some sort of gradient ascent kind of thing to locate an estimate of the posterior's peak.


```{r Quadratic Approximation of MAP}
d2


#now we use alist to bundle all of our model definitions together:

flist <- alist(
  height ~ dnorm(mu, sigma), #model definition
  mu ~ dnorm(178, 20), #prior on mu
  sigma ~ dunif(0, 50) #prior on variance/sigma
)

m4.1 <- rethinking::map(flist, data=d2)
precis(m4.1)

```

The values above give a Gaussian approximation for each parameter's marginal distribution. This means the plausibility of each value for $\mu$ averaged over all values for $\sigma$ and vice versa. The intervals here are the 89% intervals, which McElreath points out is no less arbitrary than the 95% intervals - though perhaps less 'mindless.' Also recommends, if people ask why you've chosen 89%, simplying staring at them meaningfully and saying "Because it's prime" which is an energy I, once again, love.

If we compare these intervals to the HPDI from our grid approximation - we see they are basically equivalent. When the posterior is itself Gaussian, we would expect our quadratic approximation and grid approximation to be roughly equivalent.




###### Overthinking: Starting values for map

Map does in fact climb the posterior like a hill - so some sort of gradient ascent to a quadratic approximation. But to do this, like most any other algorithm, it needs starting values. By default, map will draw random values from the prior and then start from there. But you can also specify your own starting values in a list. It is important to remember that it is a list, rather than an alist. A list evaluates the code you present, so when I say mean(d2$height), it actually calculates the mean vaue and passes that through. alist does not evaluate code, so when I pass it mu ~ dnorm(178,20), it doesn't actually draw values from dnorm or anything like that. It keeps that actual text and passes that to map when we fit the model - which is important for approximations or MCMC or HMC algorigthms. Anyways. That's all I think.

----

Now we are going to play a bit with our prior when feeding it to map. In this case, we are going to narrow the prior on $\mu$ down to 0.1 rather than 20. So we have a VERY narrow prior compared to what we had before.

```{r Narrowing our Prior}
m4.2 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178,0.1),
    sigma ~ dunif(0, 50)
  ), data = d2
)

precis(m4.2)
```
Two important things to notice here:

1. Our estimate for $\mu$ has not moved from 178, which makes sense given our prior. We basically told it only values within like 0.2 of 178 are likely and so we only really traversed that area by biasing the posterior strongly when compared to our likelihood.

2. Our estimate of $\sigma$ has ballooned significantly, even though we did not adjust its prior. This is because our estimate of each parameter is conditional on the other! So if we constrain where $/mu$ can go, $\sigma$ ends up needing to change to get a model that explains our data well. We end up with what is really an unlikely or poor estimate of $\mu$ and then a very large variance to help explain our data's variance away from that mean.


In this case our prior has strongly biased our estimate, but this is mostly because we chose a ridiculously narrow variance for it. Given the amount of data we have, our choice of prior should not have a huge impact generally - especially for such a simple model. In the future though, our priors will have a much stronger impact.

##### Overthinking: How strong is a prior?

We can estimate the strength of a prior by reimagining it as the product of previous sampling. We can do this using equation for the standard deviation, such that the $\sigma$ for our prior is treated as the $\sigma$ for a previous posterior. 

$\sigma_{post} = 1/\sqrt{n}$

So in our overly narrow prior, with 0.1, this would indicate a previous sample where $0.1 = 1/\sqrt{n}$ such that $n = 100$. So that would suggest that in a previous experiment we had a sample size of 100 with a mean height of 178. This also assumes some amount of unit variance which isn't necessarily justified by hey. Anyways, that's a very strong prior is the point. Whereas before, when we set our $\sigma = 20$, we end up with $20 = 1/\sqrt{n}$ which leads to a sample size of 0.0025. So well below a single observation. We would then consider this a very weak prior (with the assumption once again of a sum of squared variance equalling 1). Our narrow prior, with an assumed sample size of 100, would be a much stronger prior. We would need a sample of 100 with a mean of 178 etc to get that strong of a prior. So that's a nice intuitive way to examine how "strong" our priors are, thinking of them as being based on a previous sample.

---


##### 4.3.6 Sampling from a map fit

Core concept here is that getting a quadratic approximation of two parametrs, $\mu$ and $\sigma$, is a multivariate Gaussian. This leads to a slightly more complex set of summary statistics. For a univariate Gaussian, you can get by with just your mean and SD (or variance, which is just $sd^{2}$). For a multivariate Gaussian, we have covariance for each of those parameters. This decomposes into two components which are what I want to focus on: 1) a vector of variances for the parameters themselves 2) a correlation matrix that describes how a change in one parameter generates a correlated change in the other parameters.

```{r Pulling smaples from a MAP fit}

#This is the variance of our model parameters. If you take the square root of this you get the sd from our precis output earlier
diag(vcov(m4.1))

#This is our correlation matrix describing how a change in each parameter leads to a change in the others. So diagonals obviously are all 1. But then our off-diagonals are small and symmetrical - suggesting that as we change mu we get a small change in sigma and vice versa. Since it is symmetrical we get the same change regardless of which one we change. The important thing here is that these are quite small - suggesting that our knowledge of mu and sigma are largely independent. Knowing mu doesn't tell us much about sigma and vice versa. True for simple Gaussians, but as we get more complex, apparently not??
cov2cor(vcov(m4.1))


#the rethinking package provides a nice convencience function for sampling from this multivariate posterior

post <- extract.samples(m4.1, n=1e4)
head(post)
```



##### 4.4 Adding a Predictor

We now add weight to our model of height - making this a Gaussian regression. First let's plot though.

```{r Plot height ~ weight}
plot(d2$height~d2$weight)
```

There's very obviously a relationship here - knowing a person's weight gives you an idea of their height. Thus the two are non-independent. We can use one to predict the other.

###### Rethinking: What is 'regression?'

Takes it back to Galton's regression to the mean. Basically you wouldn't try and predict a son's height just from the father's height. Better to try and use the 'pool' of fathers by using the mean. We then end up with a prediction for each son but one that is 'shrunk' towards the mean. This shrinkage is typically quite robust and forms the basis of our eventual multilevel modelling approaches.

---

##### 4.4.1 The linear model strategy


Here we introduce our first linear model. Basically what this does is just treats $\mu$ as a linear function of our predictor, in this case weight. The posterior distribution the describes the relative plausibility of different strengths of the association between the two. Math-wise this is the relative plausibility of our $\beta$ coefficient. $\beta$ tells us how much $y$ changes as a function of $x$, and so tells us both the magnitude and direction of the association between them. The posterior then ranks these values based on their relative plausibility given the data and the model. So how likely is any of the infinite values of $\beta$ if we know it had to generate our data under the given model structure? That's what it's trying to answer.

How do we fit this into the old model framework? Pretty easily actually:

$h_{i} \sim Normal(\mu, \sigma)$
$\mu_{i} = \alpha + \beta x_{i}$
$\alpha \sim Normal(178, 100)$ 
$\beta \sim Normal(0, 10)$
$\sigma \sim Uniform(0,50)$

Here we've shifted a few of our priors, but overall it is more or less the same. The big change is that $\mu$ no longer has its own separate prior. It is instead a linear function of an intercept, $\alpha$, a slope describing the association, $\beta$, and our predictor, $x_{i}$. We can break this down into conceptual units as well: 

1) Likelihood: $h_{i} \sim Normal(\mu, \sigma)$
2) Linear Model: $\mu_{i} = \alpha + \beta x_{i}$
3) Intercept Prior: $\alpha \sim Normal(178, 100)$ 
4) $\beta$ Prior: $\beta \sim Normal(0, 10)$
5) $\sigma$ Prior: $\sigma \sim Uniform(0,50)$

Let's highlight the 'regression to the mean' component here, which we can see in the addition of the index $i$ to $\mu$. What this means is we no longer have a constant mean. Instead, we have a predicted mean for each individual data point. This is where we get that 'shrinkage' idea. Instead of trying to predict each individual data point, we instead try and predict an average near to it. We end up with some noise, but with good general predictive power. Also note that the linear model does not use $\sim$. It uses $=$, because the relationship between $\alpha, \beta$ and $\mu_{i}$ is not stochastic, it is deterministic. The relationship between $\mu_{i}$ and $h_{i}$ remains stochastic though, so keep that in mind.

McElreath makes a point to explain that $\alpha$ and $\beta$ are 'made up' parameters. More importantly, these are the 'targets of learning' in our model. They answer specific questions we want the model/golem to be able to answer: 

1) What is the value of $h_{i}$ when $x_{i} = 0$? This is what $\alpha$ captures for us

2) How much does $h_{i}$ change as $x_{i}$ changes? This is what our $\beta$ value describes.

As we progress, we can ask more and more nuanced questions of our golem using more complex novel parameters.

###### Overthinking: Regression and units

It is worth keeping in mind that when we add in a predictor, our units get mixed. So in our height ~ weight regression, we have $\mu$ and $\sigma$ both in cm, $\alpha$ in cm, and then weight in kg. Since weight is in kg, and $\beta$ relates our weight to height, for $\beta$ to churn out the correct units, it must be in cm/kg. That way our kg cancel. This then makes it a sort of rate. McElreath notes that there is a tradition called dimensional analysis that advocates constructing and modeling dimensionless variables. In this case, we might divide all of our data by an average height, thus removing the cm and leaving dimensionless relational values. The idea here is that since all units are really human constructions, it may be more 'natural' or 'intuitive' to model values agnostic of these idiosyncratic metrics. Also seems likely to require some wild ass abstraction to really grasp though, so who knows.

---

Lastly we have our priors. For our intercept, we once again use our author's height along with a very wide $\sigma$. This allows that intercept value to move pretty far away from that average, which is good for an intercept - particularly I would guess when we have a strong relationship like the height ~ weight regression we are working with.

$\simga$ is once again uniform and broad, and always positive. The prior on $\beta$ is the last thing that needs discussion. We center it at zero, allowing equal plausibility of a positive or negative relationship. We then give it pretty healthy spread with a $\sigma$ of 10. This isn't entirely sensible, since we can see from the plot that we have a strong positive relationship. So equally weighting negative relationships is a bit wasteful when doing a posterior sampling procedure. However, given the amount of data we have here, not a huge deal. But in other situations, where data is less plentiful and the association weaker, a prior is a good way to nudge our model towards quicker convergence on a correct value.

As it stands, these very weak priors will likely lead to similar inference as an ML regression, just with the added benefit of the posterior distribution's summary of uncertainty.

```{r Adding a predictor}
library(rethinking)
data("Howell1")
d <- Howell1

d2 <- d[d$age>=18,]

x_bar <- mean(d2$weight)

m4.3 <- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*(weight-x_bar),
    a ~ dnorm(156, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
    ),
  data=d2
)

precis(m4.3, corr = T)
cov2cor(vcov(m4.3))

#note here that our a and b estimates are almost perfectly negatively correlated. This makes sense for a simple model, as any change to our slope yields a different best fit intercept and vice versa. But if we have a more complex model that could be v bad.

#now we'll center our data by subtracting the mean from all data points.

d2$weight.c <- d2$weight - mean(d2$weight)


m4.4 <- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a+b*weight.c,
    a ~ dnorm(158,100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0,50)
  ), data=d2
)

precis(m4.4, corr=TRUE)
cov2cor(vcov(m4.4))

#so now our b is unchanged, but a is the same as the mean height in the data and the correlation among parameters has dropped to 0. What we have done is fiddled with what the intercept really is. Normally our intercept is what our outcome is when the predictor is 0. But now the mean of the predictor is 0, meaning the intercept is the value we expect for our outcome when the predictor is at its mean. Nifty. 


#plot the best fit line from model
plot(height~weight, data=d2)

abline(a=coef(m4.3)["a"], b=coef(m4.3)['b'])

#add uncertainty from posterior sampling

post <- extract.samples(m4.3)

post[1:5,]

N <- 10

dN <- d2[1:N,]



#Gonna skip to where he actually does it

mu <- link(m4.3)
str(mu)

#the link function here gives us a distribution of mu for each height in the original dataset, so a matrix with 1000 samples from the posterior for each of the 352 heights in our data.

#we want t a distribution for everything along the x axis though, so we have to generate some data then pass that to link.

weight.seq <- seq(from=25, to =70, by=1)

mu <- link(m4.3, data = data.frame(weight=weight.seq))

plot(height ~ weight, data=d2, type='n')

for(i in 1:100){
  points(weight.seq, mu[i,], pch=16, col=col.alpha(rangi2,0.1))
} 




```


Note that the above code differs from the second edition, which switches to quap rather than map. Ah well, can't win em all. IIRC, both do get the quadratic approximation of the mode of the posterior but I think map might just be older? Dunno. Docs don't really distinguish between the two but both quap and map are found in the docs for map. As in, map redirects to quap now. So maybe a deprecated older version? Who knows...

###### Overthinking: Logs and exps, oh my.

Basically McElreath notes, pretty correctly I'd say, that most people working in the life and social sciences don't think in logarithms or exponentials much. He gives a working (and very simplified) definition that $y = log(x)$ assigns y the order of magnitude of x. Like if x is in the 10,000s I assume he means that y ends up around 9.2. Going to be 100% honest here, I don't fully understand what he means here. Unsurprisingly I also don't think in logarithms (though I have gotten better with exponentials if I do say so myself). So let's dig until we at least get to the point that he's talking about.

{Alright so the logarithmic function tells you what power you would have to raise a given base to to get the value we give it. So, when I say $\log(10,000) = 9.2$, what I mean is that $e^{9.2} \approx 10,000$. If we use a different base we get a different logarithm. By default, R uses the exponential constant, $e$, as the base for the log function. Base 10 is pretty common too though and the command for that is log10. Anyways, to get back to our intuition of logarithms, when I log transform my data, I'm looking for the exponent I'd have to put on $e$ to get that value. So in a sense, yeah I am assigning an order of magnitude in base $e$ to $y$. I guess the difficulty there is that I tend to think of orders of magnitude in terms of 10, like lots of people do. So you think of going from 10 to 100 and that's an order of magnitude because thats a $10^1$ jumping to $10^2$. But if we switch bases to $e$ (natural logs for those keeping track), we end up with a slightly different intuition for what a change in order of magnitude translates to. Probably good to keep that in mind when using R's standard log function as opposed to log10. Alright, now we can keep going with his example of a log normal prior.

---

```{r Lognormal beta}

library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[d$age >=18,]
x_bar <- mean(d2$weight)

#the key difference in the model below is that our posterior distribution is no longer Beta, it is log(Beta). This is because we transform beta using our exp function, meaning that we are treating it as a logarithm. God that's a weird intuition to have to get used to but I'm sure we will. Maybe. I dunno.


m4.3b <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + exp(log_b)*(weight-x_bar),
    a ~ dnorm(178,20), #a prior for intercept informed more by our understanding of human heights than by our sample
    log_b ~ dnorm(0,1),
    sigma ~ dunif(0,50)
  ), data = d2
)
summary(m4.3b)

#so our mean slope estimate is -0.1, which when transformed back gives us about 0.9. 

```




So now we have some nice models but we don't really know how to interpret them. Or, we do know how to interpret them, looking at tables of coefficient estimates and interpreting the values reported as their impact on our response variable. Which is great! Why wouldn't you? But as we add in multiple predictors, interactions, and (shortly) the dreaded polynomials, interpreting our coefficient tables becomes much more complicated. So we have a seond approach to interpreting models, similar to what we do when testing the suitability of priors: we simulate. As a theoritician, there's nothing I love more than some simulations. So now that we have a model, we can simulate data from it, both in terms of our predicted means and even adding in noise from our sigma estimate to get simulated data.

First let's walk through our marginal distributions though:

```{r Evaluating the model - table of coefficients}
precis(m4.3)

```

Here we see our three parameters (note that it's not so different from our log_b model once we transform log_b back to b) with mean, sd, and some nice CIs. For Beta, our slope relating height and weight, we get an estimate of 0.9. We can interpret this as "a person 1 kg heavier would be, on average, 0.9 cm taller." 89% of our posterior density sits between 0.84 and 0.97 - meaning that values close to 0 or well above 1 are not compatible with our data and model. Note that we include our model in the definition of compatibility - here we have assumed a linear relationship between height and weight. We are assuming that as weight increases there is fixed amount that height should increase. This may not be the case, so, what our output really means is, so long as you are assuming a straight line defines this relationship, it's unlikely our slope is either close to 0 or well above 1.


Now that we have some parameters describing our predicted mean height, in particular $\alpha$ and $\beta$, we have to remember that we have a joint distribution for $\mu$ and subsequently we gave to keep an eye on our covariance matrix for parameter estimates.

```{r Covariance table and standard deviation}

round(vcov(m4.3),3)

sqrt(diag(round(vcov(m4.3),3)))
```

Uhhh that shouldn't be like that? His covariance matrix is quite different. In particular, $\alpha * \alpha$ covariance is like 0.07 in his and the covariance of $\alpha$ and $\beta$ is 0. Hmm not sure what to do with that tbh. Ahhh okay it was because in the original first edition he doesn't regress on the residuals of weight, weight - x_bar, he just regresses on the raw data. If we swap it out with the new second edition substitution, we get our covariance under control. Interesting. I wonder why that happens? Let's uhhh have a look on the interwebz and see what we find. Be back momentarily.

Okay didn't find much on the web cuz I'm guessing this is actually pretty intuitive. Basically, by changing our predictor to residuals, we change the magnitude they exist on. Thus we also reduce the magnitude we'd expect to see for our variances and covariances. That being said, one thing I did find on the interwebz was that, since the diagonals of our vcov matrix are the variance of the posterior for our parameter estimates, we can take the square root of those values to retrieve the standard deviations reported in our model output. That's what the second line there does. The internet claims those are the standard errors of a non-bayesian model but I don't know if that's correct. The variance would be the sum of squared deviance, which you'd have to sqrt to get the standard deviation? Then divide by square root n to get the error? But maybe it's different for parameter estimates in models? hmm......They do say its the variance of the sampling distributions so I guess if you get the SD of the samplING distro that is the standard error. Might just have to prove to myself sometime that the variance of a frequentist estimate is the variance of the sampling distro. Regardless, here we treat it as the variance in the posterior and its sqrt gives us the SD reported in the model output. Sensible.


Lmao he says at the end of this section that the lack of covariance and variance comes from centering our variables. Damn, I really should read before I type.


```{r Both together}

#this puts our coefficient estimates and vcov matrix together

pairs(m4.3)

```



##### 4.4.3.2 Plotting posterior inferenace against the data

As I mentioned earlier, just looking at tables of coefficient estimates and covariance of parameters has limited utility as we get further into modelling. So here we'll look at the other end of model evaluation which is simulating and plotting. First we'll plot our data against the predicted means of our model.

```{r Plotting samples from posterior}
plot(height ~weight, data=d2, col=rangi2)
post <- extract.samples(m4.3)
a_map <- mean(post$a)
b_map <- mean(post$b)
curve(a_map + b_map*(x-x_bar), add=T)
```

As we know, there are infinitely many lines that could fit this data, each with its own posterior probability of having generated this data given our model structure and priors. So now we'll work on adding uncertainty to our plot. 

He uses this as an opportunity to discuss how sample size also impacts certainty so I'll take that opportunity too I guess.
```{r Plotting uncertainty w/ different sample sizes}
N <- 10
N2 <- 50
N3 <- 150
N4 <-  352

dN <- d2[1:N,]
dN2<- d2[1:N2,]
dN3<- d2[1:N3,]
dN4<- d2[1:N4,]

mN <- quap(
  alist(
    height ~dnorm(mu, sigma),
    mu <- a + b*(weight-mean(weight)),
    a ~ dnorm(178,10),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ), data=dN
)


mN2 <- quap(
  alist(
    height ~dnorm(mu, sigma),
    mu <- a + b*(weight-mean(weight)),
    a ~ dnorm(178,10),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ), data=dN2
)

mN3 <- quap(
  alist(
    height ~dnorm(mu, sigma),
    mu <- a + b*(weight-mean(weight)),
    a ~ dnorm(178,10),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ), data=dN3
)

mN4 <- quap(
  alist(
    height ~dnorm(mu, sigma),
    mu <- a + b*(weight-mean(weight)),
    a ~ dnorm(178,10),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ), data=dN4
)



#now we'll extract 20 samples from the posterior for each model

postN <-  extract.samples(mN, n=20)
postN2 <-  extract.samples(mN2, n=20)
postN3 <-  extract.samples(mN3, n=20)
postN4 <-  extract.samples(mN4, n=20)

plot(dN$weight, dN$height, xlim=range(d2$weight), ylim=range(d2$height),
     col=rangi2, xlab="weight", ylab="height")
mtext(concat("N = ", N))
#now plot our posterior sample lines
for(i in 1:20)
  curve(postN$a[i]+postN$b[i]*(x-mean(dN$weight)),
        col=col.alpha("black", 0.3), add=T)



plot(dN2$weight, dN2$height, xlim=range(d2$weight), ylim=range(d2$height),
     col=rangi2, xlab="weight", ylab="height")
mtext(concat("N = ", N2))
#now plot our posterior sample lines
for(i in 1:20)
  curve(postN2$a[i]+postN2$b[i]*(x-mean(dN2$weight)),
        col=col.alpha("black", 0.3), add=T)


plot(dN3$weight, dN3$height, xlim=range(d2$weight), ylim=range(d2$height),
     col=rangi2, xlab="weight", ylab="height")
mtext(concat("N3 = ", N3))
#N3ow plot our posterior sample lines
for(i in 1:20)
  curve(postN3$a[i]+postN3$b[i]*(x-mean(dN3$weight)),
        col=col.alpha("black", 0.3), add=T)


plot(dN4$weight, dN4$height, xlim=range(d2$weight), ylim=range(d2$height),
     col=rangi2, xlab="weight", ylab="height")
mtext(concat("N4 = ", N4))
#N4ow plot our posterior sample lines
for(i in 1:20)
  curve(postN4$a[i]+postN4$b[i]*(x-mean(dN4$weight)),
        col=col.alpha("black", 0.3), add=T)



```

As we add in more data we can see that our uncertainty in the average relationship decreases. We get tighter and tighter bundling of predicted lines. So that's cool. More data = more certainty in parameter estimates conditional on our sample, model structure, and priors. Does that mean the model is good? Not necessarily. Like always, keep worrying you're wrong, right? You probably are.


Alright so now, rather than plotting a bunch of regression lines, we want to plot some contours/intervals to represent this uncertainty. First we'll work with a single x value, 50 kg. We start by extracting 10,000 values for $\alpha$ and $\beta$ from the posterior, then calculate a predicted $\mu$ at $x = 50$. This will give us a vector of 10,000 predicted means. Since each draw from the posterior gives us a sample for $\alpha$ and $\beta$ taking into account the correlation inherent in our estimates, we can look "push" our uncertainty in both parameters through into an estimate of the uncertainty in $\mu$.

```{r Constructing intervals/contours}
post <- extract.samples(m4.3)
mu_at_50 <- post$a + post$b *(50-x_bar)
```


The code alongside mu_at_50 is our actual linear model, substituting in an actual value of for $x_{i}$, 50:

$$
\mu_{i} = \alpha + \beta(x_{i} - \overline{x})
$$

We can evaluate the distribution of predicted means a couple of different ways, first as a distribution and second as intervals:
```{r Evaluating distribution of predicted means}
dens(mu_at_50, col=rangi2, lwd=2., xlab="mu|weight=50")
PI(mu_at_50, prob = 0.89)
```

Now we want to do the same thing but for every unique value in our original data, here using McElreath's very convenient link function. Link basically does what we just did but for each value used in the original model or with whatever data we provide it. in our case, we are going to provide it with a sequence of x values that span the range of weights seen in our data:

```{r Plotting prediction intervals alongside data}
weight.seq <- seq(from=25, to=70, by =1)

mu <- link(m4.3, data=data.frame(weight=weight.seq))
str(mu)


#now we'll plot our values

plot(height ~ weight, d2, type="n")

for(i in 1:100)
  points(weight.seq, mu[i,], pch=16, col=col.alpha(rangi2,0.1))


#now we'll use apply to get means and PI's for each weight value we used just now

mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob=0.89)

mu.mean
mu.PI

#now let's plot our actual data alongside our predicted invertvals

plot(height ~ weight, data = d2, col=col.alpha(rangi2, 0.5))

#plot the line using weight.seq as our x values and mu.mean as our y
lines(weight.seq, mu.mean)

#plot intervals as shaded bands around the lines using mu.PI and weight.seq
shade(mu.PI, weight.seq)

```


McElreath notes that when using the quap estimates, there are analytic solutions for intervals here. But they require an understanding of integrals (I'm guessing just integrating over uncertainty in $\alpha$ and $\beta$? Who knows) and so non stats folks often don't do well with them. That being said, he also notes that once you jump in to MCMC based modelling, which is most real modelling, those integrals are not available and so working with samples from the posterior is the only option. So may as well get used to it now. Good to know analytic approaches exist, but sampling is easier for people used to working with raw data and more versatile as you get into more advanced modelling.

Anyways, here's his overall recipe for intervals for uncertainty:

1) Use link to generate posterior distributions of $\mu$ across values for x. Keep in mind that link uses the original data by default, so if you want a custom range you gots to build it.

2) Use summary functions to to get, well, summaries, of those posterior distributions. Mean and PI are the examples he gives but presumably you could use others? Maybe mode? Go with your gut!

3) Plot the predictions using line or shade or other things to add them to your data. Get an idea of how uncertainty is spread across the range of predictor values - keeping in mind that your uncertainty here is accounting for both correlation among parameter estimates and uncertainty in those estimates.



---

###### Rethinking: Overconfident confidence intervals

Basically, keep in mind that our CI's are only as good as the model. Even a shit model can have tight intervals. Best to think of it as: conditional on the assumption that height and weight are linked by a linear function, this is the most plausible line and these are its most plausible bounds.

---



##### Overthinking: How link works

Basically, link isn't all that sophisticated, its just a wrapper for some simple calculations. More of a convenience function than something truly complicated.

```{r How link works}
#get samples for coefficients
post <- extract.samples(m4.3)
#create a function to calculate a predicted mean weight
mu.link <- function(weight) post$a + post$b*(weight- x_bar)
#create a sequence of weights
weight.seq <- seq(from=25, to = 70, by = 1)
#apply mu function to weight
mu <- sapply(weight.seq, mu.link)

#ABOVE IS ALL LINK DOES. The rest of this is what we do with link's output. So yeah, not wildly complicated but a hell of a lot better than having to homebrew a mu.link function for every model we fit I'd say. But what do I know eh? 

#get the mean
mu.mean <- apply(mu, 2, mean)
# get the CIs
mu.CI <- apply(mu, 2, PI, prob =0.89)

```


---

###### 4.4.3.5 Prediction Intervals


Now we'll generate intervals, not just for our predicted mean, but for the data itself. See below for an old note I wrote on this topic that probably is still useful. I dunno. maybe not. regardless I'm not deleting it.

Important concept here: when we move beyond just looking at uncertainty in the mean and start looking at overall uncertainty, we have to take into account that gaussian $\sigma$. Up till now, all of our posterior estimates have centered around the uncertainty in $\mu$ but not in the overall distribution of heights. So we've been looking at how does our estimate of the average vary but not at how the distribution itself is uncertain. So there's a chunk of code in the book that covers this.

So basically, now we are looking, not at the uncertainty in $\mu_{i}$, but in $h_{i}$, our predicted height. So this is draws from the posterior for:

$$
h_{i} \sim Normal (\mu_{i}, \sigma) 
$$

So now we have to predict marginal to uncertainty in $\alpha$ and $\Beta$ as they impact $\mu_{i}$, along with uncertainty in our estimate of $\sigma$. Lots of uncertainty here, and we're still just thinking about a single predictor of a simple linear model - no GLMs or group variables. WILD!

We'll be using the sim function, I'm guessing once again from McElreath's rethinking package.

```{r Simulating prediction intervals for heights}
library(rethinking)
sim.height <- sim(m4.3, data=list(weight=weight.seq), n=1e4)
str(sim.height)

#now we'll summarize our heights with PIs

height.PI <- apply(sim.height, 2, PI, prob=0.89)

#now let's plot all three of our predictions, means, intervals around means, and intervals around data:

plot(height~weight, d2, col=col.alpha(rangi2,0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)

```

Absolutely stunning, we love to see it. Look how few data points fall outside our 89% prediction intervals. A true thing of statistical beauty. Awestruck. Anyways.

Quick note that the sim function above takes an n argument. McElreath points out that the higher the n argument for your simulated heights, the smoother the intervals are at their edges. I bumped it up to 10,000 samples above, just because I could, and it really did smooth them out a fair amount.

---

###### Rethinking: Two kinds of Uncertainty

I've talked a bit above about the various sources of uncertainty in the predictions we just plotted. But it's worth noting, as McElreath does, that predicting heights, our actual data, incorporates a separate type of uncertainty or error. That's modelling the process associated with sampling variance in a Gaussian process - what generates the symmetrical spread of data around the mean. That's another modelling assumption inherent to choosing a Gaussian response, but it represents a different type of uncertainty which may be epistemological (more of a convenience for getting predictions) or ontological (a prediction about future data). In a sense, this could have either  "small world" or "large world" implications depending on your modelling approach and rationale.

---

Very quickly, to be consistent, I'll walk through what sim does like I did for link. Also in the book, but nice to have it here in a markdown document I guess.

```{r }

weight.seq <- 25:70

#These lines here are actually what sim does. Extract samples from the posterior then feed those values into rnorm using our linear model for the mean and the posterior samples of sigma for the standard deviation. Yet again, quite simple, but would be very annoying to write it over and over again for every new model. So sim acts as a convenience function to pull the model formula out of the model object (I'm guessing), then feed the data and samples into rnorm in the correct format. Nifty.
post <- extract.samples(m4.3)
sim.height <- sapply(weight.seq, function(weight){
  rnorm(
    n=nrow(post),
    mean=post$a+post$b*(weight-x_bar),
    sd=post$sigma
  )
})

height.PI <- apply(sim.height, 2, PI, prob=0.89)
```





### 4.5 Curves from Lines

Finally! A new section. God these sections got long in the second edition. I mean good for Richard, like great content very thorough, but damn I got so many headers in this markdown now.

Looks like this section is going to cover both polynomial regression and B-splines. I'm familiar with polynomials but I've never really gotten splines so pretty pumped to have a nice explanation. As McElreath puts it, both just act as transformations of your predictors into synthetic variables with a better (nonlinear) relationship to your response variable. Apparently splines are better but who knows??


```{r}
library(rethinking)
data("Howell1")
d <- Howell1

plot(d$height~d$weight)

d$weight.s <- ( d$weight - mean(d$weight) )/sd(d$weight)
```


As ever, polynomial regression is dicey. They are very difficult to interpret, and there's the whole issue of highly correlated predictors. But here we go anyways!

$\mu_{i} = \alpha + \beta_{1} x_{i} + \beta_{2} x_{i}^2$

This is a parabolic function, a second order function of x. Here $\beta_{2}$ measures the strength of that curve.

We start by standardizing our variable. This helps control for scale and get an idea of the relative importance of each predictor. Downside is of course interpretation or prediction on the natural scale of the predictor.






Just gonna jump straight into splines tbh.

```{r}
#First let's construct our basis functions. This will be a cubic basis function with 15 knots, using the cherry blossom bloom data included in rethinking.

library(rethinking)

data(cherry_blossoms)
d <- cherry_blossoms

precis(d)

#now just complete cases, then create quantiles that will define where each knot begins. 

d2 <- d[complete.cases(d$doy),]
num_knots <- 15
knot_list <- quantile(d2$year, probs=seq(0,1,length.out=num_knots))


#now we construct the basis functions using the splines package
library(splines)
B <- bs(d2$year,
        knots=knot_list[-c(1,num_knots)],
        degree=3, intercept=T)

B

#now let's plot them, although they aren't always the most intuitive to read tbh

plot(NULL, xlim=range(d2$year), ylim=c(0,1), xlab="year", ylab="basis")
for(i in 1:ncol(B))lines(d2$year, B[,i])


#Alright so basically, each row in B is a year and each column the value that year takes for a given basis function. We have 17 columns, so our fifteen knots plus the beginning
#and ending range right? Like if we have 15 knots, we have 17 ranges where importance of each basis changes. So at the start, the only non-zero values are for the first 1:4 
#basis functions. So over the first range of years, like the first 200 or so years, we are only looking at our first four or so basis functions. As we progress through that
#we start to include the other basis functions - their values increasing as the values in columns 1:3 decrease. So when we start adding weights, we're weighting each
#combination of basis functions different. Gonna need a lot of time to grok this nonsense but it intuitively feels very powerful? If not just because it feels very similar
#to lots of other nonlinear methods like GAMs and Fourier transforms. Basically we have some highly nonlinear relationship between y~x, and we model it as the sum of non-linear 
#relationships. Because of how many basis functions we have, each of which are technically identical, when we sum them and weight each sum differently, we end up with an #increasingly non-linear relationship between y and x. Really need to think hard about how our prediction changes as a function of both basis degree and weights. But overall
#seems like a very powerful and seemingly widely used process.

#Alright so the basis functions themselves range from 0 to 1. So each basis result in B can give me only some value between 0 and 1. We then sum these for each of our potential
#years. So that each predicted value, before our weight, I would think has some fixed value? Looks like they all sum to 1? Makes some amount of sense...And this is standardized/
#scaled deviance from mean year. So that has some innate interpretation I would think - not sure what though.
d2$year
sum(B[1,])
B[1,]
B[2,]
sum(B[2,])



```


